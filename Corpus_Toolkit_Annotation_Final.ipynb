{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOytq1J7e5s1ukwlMJB29WM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Class_Activities/blob/main/Corpus_Toolkit_Annotation_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Corpus (코퍼스/말뭉치) \n",
        "- 자연어 연구에 사용되고 형태소 분석을 사전에 시행하여 분석에 정확성을 기한다. 코퍼스 분석은 단어수 계산, n-gram 빈도수와 범워, 키값, 연어(collocation), 의존합자식별(예: 동사-직접목적어 조합) 및 의존합자식별 빈도-범위-연결강도 분석을 한다. \n",
        "\n",
        "- 위의 분석을 위해서는 아래 패키지를 설치한다 \n",
        "-- corpus-toolkit : tokenization(토큰화) & lemmatization(표준형태화) 포함\n",
        "- 참조: Kyle 의 코드에는 Spacy 패키지(tagging(단어에 문법범주 연결) & parsing(구분자를 사용하여 문자열을 구성 요소로 분석))를 포함 하였으나, 여기에서는 Spacy 패키지를 따로 설치하지 않고, nltk에서 사용하는 코드를 사용할 예정.  \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5"
      },
      "outputs": [],
      "source": [
        "pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing (사전처리)\n",
        "- 코퍼스 분석을 하려면 분석하려는 텍스트의 단어를 토큰화하여 분석할 수 있는 형태로 준비해 놓아야 한다. i) tokenize 함수를 사용하여 토큰을 리스트화 하여 준비해 놓을 수도 있고, ii) tag 함수를 사용하여 토큰에 문법범주(품사)를 연결해서 준비해 놓을 수도 있다.  \n",
        "-- [코드문법1] corpus_toolkit패키지에서 corpus_tools모듈을 ct로 줄여서 불러들여라.\n",
        "-- [코드문법2] [파일 올리고 읽기] ct모듈의 ldcorpus( )함수에 brwon_single 데이터 폴더를 인자로 넣어, 그 결과를 brown_corp변수에 할당한다.\n",
        "-- [코드문법3] [단어 토큰화] ct모듈의 tokenize( )함수 인자로 바로 위 코드 변수 brown_corp를 넣어, 그 결과를 tok_corp변수에 할당한다.\n",
        "-- [코드문법4] [토큰화한 단어 빈도수] ct모듈의 frequency( )함수 인자로 바로 위 코드 변수 tok_corp를 넣어, 그 결과를 brown_freq변수에 할당한다.\n",
        "-- [코드문법5] ct모듈의 head( )에 brown_freq변수를 첫 째 인자로, 빈도수가 높은 것 10개를 둘 째 인자로 사용해서, 상위 10개 토큰의 빈도수를 출력한다."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab 작업 디렉토리에서 폴더를 생성하여 다른 곳에서 다운로드 받은 단/복수의 파일을 옮겨 넣는 방법 \n",
        "- 코랩에 기본으로 깔려 있는 sample_data 에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 거기를 클릭하면 선택사항 -> new folder를 클릭한다. \n",
        "- new folder를 drag해서 sample_data 쪽으로 끌어오면 같은 레벨로 폴더가 이동한다. \n",
        "- new folder에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다.Rename folder 클릭해서 파일명을 새이름으로 바꾼다. 예)brown_single\n",
        "- 해당 폴더에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다.Upload 클릭해서 본인 컴퓨터에 다운로드 받은 복수의 파일을 모두 업로드 한다. "
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 명령어 차곡히 쌓기 (Nest) \n",
        "위에서 파일을 로드해서 읽고 -> 토큰화 하고 -> 토큰의 빈도수를 계산하는 명령어를 사용하여 코드로 작성했다. 아래 코드와 같이 모듈.함수( )인 명령어를 차곡차곡 쌓아서 같은 목적을 달성할 수 있고 이를 추천한다. "
      ],
      "metadata": {
        "id": "R-98Zbi6j8qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "id": "zxrWoaKbj-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "_z3M-DivngQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"quick\",\"quickly\"], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "AZRzHbwUnjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results3 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run.*\",\"ran\"],collocates = [\"quick.*\"], nhits = 10, regex = True)\n",
        "for x in conc_results3:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "Bf2gOwCnoDCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write concordance lines to a file called \"run_25.txt\"\n",
        "conc_results4 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"], outname = \"run_25.txt\")"
      ],
      "metadata": {
        "id": "MCU4o-D4oQo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_brown = ct.tag(ct.ldcorpus(\"brown_single\"))\n",
        "ct.write_corpus(\"tagged_brown_single\",tagged_brown) #the first argument is the folder where the tagged files will be written"
      ],
      "metadata": {
        "id": "q_HExln_qlMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct.write_corpus(\"tagged_brown_single\",ct.tag(ct.ldcorpus(\"brown_single\")))"
      ],
      "metadata": {
        "id": "ubYWE2zDsvYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_freq = ct.frequency(ct.reload(\"tagged_brown_single\"))\n",
        "ct.head(tagged_freq, hits = 10)"
      ],
      "metadata": {
        "id": "syhi8MT0s7xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "id": "TWCovRiTtJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First, generate frequency lists for each corpus\n",
        "corp1freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp1\")))\n",
        "corp2freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp2\")))\n",
        "\n",
        "#then calculate Keyness\n",
        "corp_key = ct.keyness(corp1freq,corp2freq, effect = \"log-ratio\")\n",
        "ct.head(corp_key, hits = 10) #to display top hits\n"
      ],
      "metadata": {
        "id": "HcUhyymttUi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##N-grams\n",
        "N-grams are contiguous sequences of n words. The tokenize() function can be used to create an n-gram version of a corpus by employing the ngram argument. By default, words in an n-gram are separated by two underscores \"__\""
      ],
      "metadata": {
        "id": "AC3sbfm_uRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\n",
        "ct.head(trigramfreq, hits = 10)"
      ],
      "metadata": {
        "id": "Ll9zKBO4uAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependency bigrams \n",
        "Dependency bigrams consist of two words that are syntactically connected via a head-dependent relationship. For example, in the clause \"The player kicked the ball\", the main verb kicked is connected to the noun ball via a direct object relationship, wherein kicked is the head and ball is the dependent.\n",
        "\n",
        "The function dep_bigram() generates frequency dictionaries for the dependent, the head, and the dependency bigram. In addition, range is calculated along with a complete list of sentences in which the relationship occurs."
      ],
      "metadata": {
        "id": "js4OUwWuubZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bg_dict = ct.dep_bigram(ct.ldcorpus(\"brown_single\"),\"dobj\")\n",
        "ct.head(bg_dict[\"bi_freq\"], hits = 10)\n",
        "#other keys include \"dep_freq\", \"head_freq\", and \"range\"\n",
        "#also note that the key \"samples\" can be used to obtain a list of sample sentences\n",
        "#but, this is not compatible with the ct.head() function (see ct.dep_conc() instead)"
      ],
      "metadata": {
        "id": "BJy-ilc-uf4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Strength of association\n",
        "Various measures of strength of association can calculated between dependents and heads. The soa() function takes a dictionary generated by the dep_bigram() function and calculates the strength of association for each dependency bigram."
      ],
      "metadata": {
        "id": "UewcMQUOuxRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soa_mi = ct.soa(bg_dict,stat = \"MI\")\n",
        "#other stat options include: \"T\", \"faith_dep\", \"faith_head\",\"dp_dep\", and \"dp_head\"\n",
        "ct.head(soa_mi, hits = 10)"
      ],
      "metadata": {
        "id": "CNvuN00Cu1Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Concordance lines for dependency bigrams\n",
        "\n",
        "A number of excellent cross-platform GUI- based concordancers such as AntConc are freely available, and are likely the preferred method for most concordancing.\n",
        "\n",
        "However, it is difficult to get concordance lines for dependency bigrams without a more advanced program. The dep_conc() function takes the samples generated by the dep_bigram() function and creates a random sample of hits (50 hits by default) formatted as an html file.\n",
        "\n",
        "The following example will write an html file named \"dobj_results.html\" to your working directory."
      ],
      "metadata": {
        "id": "G9iry7cpvEbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct.dep_conc(bg_dict[\"samples\"],\"dobj_results\")"
      ],
      "metadata": {
        "id": "Nxh0aFGou9cP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}